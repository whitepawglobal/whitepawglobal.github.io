<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Deepfake Audio with Wav2Lip -</title><meta name=Description content="Step-by-step walkthrough on lip-syncing with Wav2Lip"><meta property="og:title" content="Deepfake Audio with Wav2Lip"><meta property="og:description" content="Step-by-step walkthrough on lip-syncing with Wav2Lip"><meta property="og:type" content="article"><meta property="og:url" content="/deepfake-audio-with-wav2lip/"><meta property="og:image" content="/deepfake-audio-with-wav2lip/featured-image.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-10T00:00:00+00:00"><meta property="article:modified_time" content="2022-10-10T00:00:00+00:00"><meta property="og:site_name" content="Codenamewei Tech Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/deepfake-audio-with-wav2lip/featured-image.jpg"><meta name=twitter:title content="Deepfake Audio with Wav2Lip"><meta name=twitter:description content="Step-by-step walkthrough on lip-syncing with Wav2Lip"><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=/deepfake-audio-with-wav2lip/><link rel=prev href=/startup-to-corporate/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Deepfake Audio with Wav2Lip","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"\/deepfake-audio-with-wav2lip\/"},"image":[{"@type":"ImageObject","url":"\/deepfake-audio-with-wav2lip\/featured-image.jpg","width":1785,"height":519}],"genre":"posts","keywords":"deepfake, lipsync, python","wordcount":1037,"url":"\/deepfake-audio-with-wav2lip\/","datePublished":"2022-10-10T00:00:00+00:00","dateModified":"2022-10-10T00:00:00+00:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"","logo":{"@type":"ImageObject","url":"\/images\/avatar.png","width":1266,"height":1266}},"author":{"@type":"Person","name":"Chiawei Lim"},"description":"Step-by-step walkthrough on lip-syncing with Wav2Lip"}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title>Codenamewei Tech Blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/categories/documentation/>Docs </a><a class=menu-item href=/about/>About </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title>Codenamewei Tech Blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/categories/documentation/ title>Docs</a><a class=menu-item href=/about/ title>About</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Deepfake Audio with Wav2Lip</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://www.linkedin.com/in/codenamewei/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Chiawei Lim</a></span>&nbsp;<span class=post-category>included in <a href=/categories/nlp/><i class="far fa-folder fa-fw" aria-hidden=true></i>nlp</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2022-10-10>2022-10-10</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1037 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;5 minutes&nbsp;</div></div><div class=featured-image><img class=lazyload src=/svg/loading.min.svg data-src=/deepfake-audio-with-wav2lip/featured-image.jpg data-srcset="/deepfake-audio-with-wav2lip/featured-image.jpg, /deepfake-audio-with-wav2lip/featured-image.jpg 1.5x, /deepfake-audio-with-wav2lip/featured-image.jpg 2x" data-sizes=auto alt=/deepfake-audio-with-wav2lip/featured-image.jpg title="Step-by-step walkthrough on lip-syncing with Wav2Lip"></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><ul><li><a href=#overview-of-deepfake-technology>Overview of Deepfake Technology</a></li><li><a href=#deepfake-audio-with-wav2lip>Deepfake Audio with Wav2Lip</a><ul><li><a href=#environment-setup>Environment Setup</a></li><li><a href=#preparation-of-files>Preparation of files</a><ul><li><a href=#model-file>Model file</a></li><li><a href=#video-file>Video file</a></li><li><a href=#audio-file>Audio file</a></li></ul></li><li><a href=#main-running-step>Main Running Step</a></li></ul></li><li><a href=#notes-updated-on-10th-october-2022>Notes (Updated on 10th October 2022)</a></li></ul></li></ul></nav></div></div><div class=content id=content><h3 id=overview-of-deepfake-technology>Overview of Deepfake Technology</h3><p>Deepfake is a technology that creates synthesis media with a subfield of Machine Learning - Deep Learning.</p><figure><a class=lightgallery href=/deepfake-audio-with-wav2lip/0.jpeg title=/deepfake-audio-with-wav2lip/0.jpeg data-thumbnail=/deepfake-audio-with-wav2lip/0.jpeg data-sub-html="<h2>Photo by the author</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/deepfake-audio-with-wav2lip/0.jpeg data-srcset="/deepfake-audio-with-wav2lip/0.jpeg, /deepfake-audio-with-wav2lip/0.jpeg 1.5x, /deepfake-audio-with-wav2lip/0.jpeg 2x" data-sizes=auto alt=/deepfake-audio-with-wav2lip/0.jpeg width=1600 height=894></a><figcaption class=image-caption>Photo by the author</figcaption></figure><p>A common use case that the public is aware of is the application of face-swapping. A target face is swapped and merged, often seamlessly on first glance view, to create an altered event.</p><figure><a class=lightgallery href=/deepfake-audio-with-wav2lip/1.jpeg title=/deepfake-audio-with-wav2lip/1.jpeg data-thumbnail=/deepfake-audio-with-wav2lip/1.jpeg data-sub-html="<h2>Photo by the author</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/deepfake-audio-with-wav2lip/1.jpeg data-srcset="/deepfake-audio-with-wav2lip/1.jpeg, /deepfake-audio-with-wav2lip/1.jpeg 1.5x, /deepfake-audio-with-wav2lip/1.jpeg 2x" data-sizes=auto alt=/deepfake-audio-with-wav2lip/1.jpeg width=1340 height=642></a><figcaption class=image-caption>Photo by the author</figcaption></figure><p>On a high level, Deepfake can be split into 3 sub-domains based on the focus of the media to alter. The former-mentioned use case (face-swapping) falls under Deepfake vision, where the image or video streams were targeted.</p><figure><a class=lightgallery href=/deepfake-audio-with-wav2lip/2.jpeg title=/deepfake-audio-with-wav2lip/2.jpeg data-thumbnail=/deepfake-audio-with-wav2lip/2.jpeg data-sub-html="<h2>Photo by the author</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/deepfake-audio-with-wav2lip/2.jpeg data-srcset="/deepfake-audio-with-wav2lip/2.jpeg, /deepfake-audio-with-wav2lip/2.jpeg 1.5x, /deepfake-audio-with-wav2lip/2.jpeg 2x" data-sizes=auto alt=/deepfake-audio-with-wav2lip/2.jpeg width=1600 height=754></a><figcaption class=image-caption>Photo by the author</figcaption></figure><p>On the other hand, Deepfake audio clone speech from third-party sources to the person in interest. In a scenario where one only communicates through phone calls, one might not be able to tell the authenticity of the sound.</p><figure><a class=lightgallery href=/deepfake-audio-with-wav2lip/3.jpeg title=/deepfake-audio-with-wav2lip/3.jpeg data-thumbnail=/deepfake-audio-with-wav2lip/3.jpeg data-sub-html="<h2>Photo by the author</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/deepfake-audio-with-wav2lip/3.jpeg data-srcset="/deepfake-audio-with-wav2lip/3.jpeg, /deepfake-audio-with-wav2lip/3.jpeg 1.5x, /deepfake-audio-with-wav2lip/3.jpeg 2x" data-sizes=auto alt=/deepfake-audio-with-wav2lip/3.jpeg width=1430 height=615></a><figcaption class=image-caption>Photo by the author</figcaption></figure><p>To achieve the impersonation both visually and audibly, one would have to not only look like the desired target but also sound like him. An example is shown below, where a famous actor, Morgan Freeman, is portrayed with a combination of Deepfake audio and Deepfake vision.</p><figure><a class=lightgallery href=/deepfake-audio-with-wav2lip/youtube_0.jpg title=/deepfake-audio-with-wav2lip/youtube_0.jpg data-thumbnail=/deepfake-audio-with-wav2lip/youtube_0.jpg data-sub-html="<h2>Play the Youtube video at: https://bit.ly/3yI23zN</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/deepfake-audio-with-wav2lip/youtube_0.jpg data-srcset="/deepfake-audio-with-wav2lip/youtube_0.jpg, /deepfake-audio-with-wav2lip/youtube_0.jpg 1.5x, /deepfake-audio-with-wav2lip/youtube_0.jpg 2x" data-sizes=auto alt=/deepfake-audio-with-wav2lip/youtube_0.jpg width=967 height=536></a><figcaption class=image-caption>Play the Youtube video at: <a href=https://bit.ly/3yI23zN target=_blank rel="noopener noreffer">https://bit.ly/3yI23zN</a></figcaption></figure><h3 id=deepfake-audio-with-wav2lip>Deepfake Audio with Wav2Lip</h3><h4 id=environment-setup>Environment Setup</h4><p>This article focuses on Deepfake Audio with the implementation from Github repository <a href=https://github.com/Rudrabha/Wav2Lip target=_blank rel="noopener noreffer">https://github.com/Rudrabha/Wav2Lip</a>. The repository is based on the paper <em>A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild</em> published at ACM Multimedia 2020.</p><p>The setup requires to run the repository includes</p><p><strong>1. Python 3.6</strong></p><p>Create a new virtual python environment with your preferred methods. In this example, conda command is illustrated.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>conda create -n wav2lip python=3.6
</span></span></code></pre></td></tr></table></div></div><p><strong>2. Ffmpeg</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>pip install ffmpeg-python
</span></span></code></pre></td></tr></table></div></div><p>Particularly in Linux OS, the following installation method is preferred.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>sudo apt-get install ffmpeg
</span></span></code></pre></td></tr></table></div></div><p><strong>3. Installation of requirements.txt from the repository</strong></p><p>In this step, the repository is first cloned with the command below.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>git clone https://github.com/Rudrabha/Wav2Lip
</span></span></code></pre></td></tr></table></div></div><div class="details admonition note open"><div class="details-summary admonition-title"><i class="icon fas fa-pencil-alt fa-fw" aria-hidden=true></i>Note<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>Note: In the main execution step, the python script in the repository is necessary for the actual merging of audio and video streams.
Packages required, as layout in the file requirements.txt, are installed with the command below.</div></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>pip install -r requirements.txt
</span></span></code></pre></td></tr></table></div></div><h4 id=preparation-of-files>Preparation of files</h4><p>Largely three data source inputs are required in order to run the lip-sync with Wav2Lip.</p><figure><a class=lightgallery href=/deepfake-audio-with-wav2lip/4.jpeg title=/deepfake-audio-with-wav2lip/4.jpeg data-thumbnail=/deepfake-audio-with-wav2lip/4.jpeg data-sub-html="<h2>Photo by the author</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/deepfake-audio-with-wav2lip/4.jpeg data-srcset="/deepfake-audio-with-wav2lip/4.jpeg, /deepfake-audio-with-wav2lip/4.jpeg 1.5x, /deepfake-audio-with-wav2lip/4.jpeg 2x" data-sizes=auto alt=/deepfake-audio-with-wav2lip/4.jpeg width=898 height=402></a><figcaption class=image-caption>Photo by the author</figcaption></figure><h5 id=model-file>Model file</h5><p>Two models are supported out-of-the box:</p><ul><li><a href="https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?e=TBFBVW" target=_blank rel="noopener noreffer">Wav2Lip</a></li><li><a href="https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?e=TBFBVW" target=_blank rel="noopener noreffer">Wav2Lip GAN</a></li></ul><p>While only one model is necessary for a single run, do download both for a comparison of the results.</p><h5 id=video-file>Video file</h5><p>For a desirable output, the input video should fulfill the following conditions.</p><ul><li>Person of interest centered and upfront in the video</li><li>Person of interest with restricted body gesture and minimum head movement</li><li>Person of interest with no occlusions arounds the face region (mask, shadows, hair)</li><li>Preferably person of interest with still lip-movement</li><li>Good lighting with not cluttered background</li><li>Video with good resolution</li></ul><div class="details admonition tip open"><div class="details-summary admonition-title"><i class="icon fas fa-lightbulb fa-fw" aria-hidden=true></i>Tip<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>Particularly, it has been found that the results are promising when the person of interest has no lip movement. This is because, through the algorithm, the mouth area would be altered to match the intonation of the speech. Having any movement from the source complicates the alteration and it might present unnatural results with visible glitches.</div></div></div><h5 id=audio-file>Audio file</h5><p>For a desirable output, the input audio should fulfill the following conditions.</p><ul><li>Clear and succinct single source of audio (Do not have more than one person conversing with overlapping speech)</li><li>Minimum/no background noises</li></ul><div class="details admonition tip open"><div class="details-summary admonition-title"><i class="icon fas fa-lightbulb fa-fw" aria-hidden=true></i>Tip<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>As the idea is to merge the audio script to the person in interest in the video, both input files should be of similar lengths in time.</div></div></div><h4 id=main-running-step>Main Running Step</h4><p>With the files prepared and saved in local paths, it’s time to run the Wav2Lip using the script inference.py. The script is located in the path of <code>&lt;path-to>\Wav2Lip\inference.py</code>.</p><p>Command should be constructed in the following manner:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>python inference.py --checkpoint_path &lt;path-to-model-file&gt; --face &lt;path-to-video-file&gt; --audio &lt;path-to-audio-file&gt;
</span></span></code></pre></td></tr></table></div></div><p>Example of command for better clarity:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>python inference.py --checkpoint_path C:\Users\codenamewei\deepfake\model\wav2lip_gan.pth --face C:\Users\codenamewei\Documents\deepfake\data\video_short.mp4 --audio C:\Users\codenamewei\Documents\deepfake\data\audio_short.wav
</span></span></code></pre></td></tr></table></div></div><p>The default and frequently used parameters are layouts as below.</p><p>For a full list, check out the <a href=https://github.com/Rudrabha/Wav2Lip/blob/master/inference.py#L13-L51 target=_blank rel="noopener noreffer">parameters settings from the source script.</a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>--checkpoint_path:
</span></span><span class=line><span class=cl>Path to checkpoint file of supported model (Wav2Lip, Wav2Lip GAN)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>--face:
</span></span><span class=line><span class=cl>Path to video file of supported format (mp4, avi, ...)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>--audio:
</span></span><span class=line><span class=cl>Path to audio file of supported format (wav, flac, ...)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>(Optional) --nosmooth:
</span></span><span class=line><span class=cl>When set, this results in no smoothing around the mouth region
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>(Optional) --pads:
</span></span><span class=line><span class=cl>When set, this adjust the range of the region to alter
</span></span><span class=line><span class=cl>Parameters set in this manner: (top, bottom, left, right)
</span></span><span class=line><span class=cl>Example:
</span></span><span class=line><span class=cl>--pads [0, 20, 0, 0]
</span></span><span class=line><span class=cl>The example above adds 20 pixels to the chin area
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>(Optional) --static:
</span></span><span class=line><span class=cl>When set to true, only the first frame will be used.
</span></span><span class=line><span class=cl>Example:
</span></span><span class=line><span class=cl>--static True
</span></span></code></pre></td></tr></table></div></div><p>If you are questioning which model file to use, the readme.md in the repository points out the key properties of each model as shown in the screenshot below.</p><figure><a class=lightgallery href=/deepfake-audio-with-wav2lip/modelcomparison.png title=/deepfake-audio-with-wav2lip/modelcomparison.png data-thumbnail=/deepfake-audio-with-wav2lip/modelcomparison.png data-sub-html="<h2>Photo by the author. Screenshot from Rudrabha/Wav2Lip repository.</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/deepfake-audio-with-wav2lip/modelcomparison.png data-srcset="/deepfake-audio-with-wav2lip/modelcomparison.png, /deepfake-audio-with-wav2lip/modelcomparison.png 1.5x, /deepfake-audio-with-wav2lip/modelcomparison.png 2x" data-sizes=auto alt=/deepfake-audio-with-wav2lip/modelcomparison.png width=690 height=233></a><figcaption class=image-caption>Photo by the author. Screenshot from Rudrabha/Wav2Lip repository.</figcaption></figure><p>Wav2Lip better mimics the mouth movement to the utterance sound, and Wav2Lip + GAN creates better visual quality.</p><figure><a class=lightgallery href=/deepfake-audio-with-wav2lip/5.jpeg title=/deepfake-audio-with-wav2lip/5.jpeg data-thumbnail=/deepfake-audio-with-wav2lip/5.jpeg data-sub-html="<h2>Photo by the author</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/deepfake-audio-with-wav2lip/5.jpeg data-srcset="/deepfake-audio-with-wav2lip/5.jpeg, /deepfake-audio-with-wav2lip/5.jpeg 1.5x, /deepfake-audio-with-wav2lip/5.jpeg 2x" data-sizes=auto alt=/deepfake-audio-with-wav2lip/5.jpeg width=1600 height=298></a><figcaption class=image-caption>Photo by the author</figcaption></figure><p>To further understand what it means, check out the example below captured in the same time stamp. One noticeable difference seen in the screenshot below is that Wav2Lip + GAN creates a more visually appealing result, where the teeth are rendered in a natural manner.</p><figure><a class=lightgallery href=/deepfake-audio-with-wav2lip/6.jpeg title=/deepfake-audio-with-wav2lip/6.jpeg data-thumbnail=/deepfake-audio-with-wav2lip/6.jpeg data-sub-html="<h2>Photo by the author</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/deepfake-audio-with-wav2lip/6.jpeg data-srcset="/deepfake-audio-with-wav2lip/6.jpeg, /deepfake-audio-with-wav2lip/6.jpeg 1.5x, /deepfake-audio-with-wav2lip/6.jpeg 2x" data-sizes=auto alt=/deepfake-audio-with-wav2lip/6.jpeg width=1265 height=642></a><figcaption class=image-caption>Photo by the author</figcaption></figure><p>This tutorial shows the side-by-side comparison of the end results from the implementation.</p><figure><a class=lightgallery href=/deepfake-audio-with-wav2lip/youtube_1.jpg title=/deepfake-audio-with-wav2lip/youtube_1.jpg data-thumbnail=/deepfake-audio-with-wav2lip/youtube_1.jpg data-sub-html="<h2>Play the youtube video at: https://bit.ly/3MGaCkL</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/deepfake-audio-with-wav2lip/youtube_1.jpg data-srcset="/deepfake-audio-with-wav2lip/youtube_1.jpg, /deepfake-audio-with-wav2lip/youtube_1.jpg 1.5x, /deepfake-audio-with-wav2lip/youtube_1.jpg 2x" data-sizes=auto alt=/deepfake-audio-with-wav2lip/youtube_1.jpg width=1372 height=775></a><figcaption class=image-caption>Play the youtube video at: <a href=https://bit.ly/3MGaCkL target=_blank rel="noopener noreffer">https://bit.ly/3MGaCkL</a></figcaption></figure><h3 id=notes-updated-on-10th-october-2022>Notes (Updated on 10th October 2022)</h3><p>To test the implementation of Deepfake audio, two GitHub repositories were under consideration:</p><ul><li><a href=https://github.com/Rudrabha/Wav2Lip target=_blank rel="noopener noreffer">https://github.com/Rudrabha/Wav2Lip</a> (Demonstrated in this article)</li><li><a href=https://github.com/Markfryazino/wav2lip-hq target=_blank rel="noopener noreffer">https://github.com/Markfryazino/wav2lip-hq</a></li></ul><p>A promising result is shown with the out-of-the-box model of Wav2Lip. The HD model mentioned in the repository likely will deliver higher performance results.</p><p>For Wav2Lip-hq, a noticeable distortion of colors on the result can be seen (as shown in the picture below). The same concern was raised in several issues as well. <a href=https://github.com/Markfryazino/wav2lip-hq/issues/6 target=_blank rel="noopener noreffer">#6</a>, <a href=https://github.com/Markfryazino/wav2lip-hq/issues/11 target=_blank rel="noopener noreffer">#11</a>. Due to that, it’s very unlikely to leverage the results generated from the repository.</p><figure><a class=lightgallery href=/deepfake-audio-with-wav2lip/7.jpeg title=/deepfake-audio-with-wav2lip/7.jpeg data-thumbnail=/deepfake-audio-with-wav2lip/7.jpeg data-sub-html="<h2>Photo by the author</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/deepfake-audio-with-wav2lip/7.jpeg data-srcset="/deepfake-audio-with-wav2lip/7.jpeg, /deepfake-audio-with-wav2lip/7.jpeg 1.5x, /deepfake-audio-with-wav2lip/7.jpeg 2x" data-sizes=auto alt=/deepfake-audio-with-wav2lip/7.jpeg width=1370 height=475></a><figcaption class=image-caption>Photo by the author</figcaption></figure><p>Thanks for reading!</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-10-10</span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/deepfake-audio-with-wav2lip/index.md target=_blank>Read Markdown</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=/deepfake-audio-with-wav2lip/ data-title="Deepfake Audio with Wav2Lip" data-via=codenamewei_ data-hashtags=deepfake,lipsync,python><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=/deepfake-audio-with-wav2lip/ data-hashtag=deepfake><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=/deepfake-audio-with-wav2lip/ data-title="Deepfake Audio with Wav2Lip"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=/deepfake-audio-with-wav2lip/ data-title="Deepfake Audio with Wav2Lip"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.0.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=/deepfake-audio-with-wav2lip/ data-title="Deepfake Audio with Wav2Lip"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/deepfake/>deepfake</a>,&nbsp;<a href=/tags/lipsync/>lipsync</a>,&nbsp;<a href=/tags/python/>python</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/startup-to-corporate/ class=prev rel=prev title="The Epiphany: Moving from A Startup to A 100 Years Old Corporate"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>The Epiphany: Moving from A Startup to A 100 Years Old Corporate</a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.111.3">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank></a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},lightgallery:!0,search:{algoliaAppID:"PASDMWALPK",algoliaIndex:"index.en",algoliaSearchKey:"b42948e51daaa93df92381c8e2ac0f93",highlightTag:"em",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"algolia"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>